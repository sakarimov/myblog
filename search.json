[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Just Another Attempt to Share",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Name : Sulthan A. Karimov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning - Supervised Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Name : Sulthan A. Karimov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Name : Sulthan A. Karimov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport the necessary packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsalut dengan bangsa indonesia\n\n\n\n\n\n\nopini\n\n\nngelantur\n\n\nindonesia\n\n\n\n\n\n\n\n\n\nJun 29, 2024\n\n\nsulthan a. karimov\n\n\n\n\n\n\n\n\n\n\n\n\nmy first encounter with webrtc and livekit agents\n\n\n\n\n\n\nbackend\n\n\npython\n\n\nai\n\n\nmlops\n\n\nwebrtc\n\n\ncloud\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nsulthan a. karimov\n\n\n\n\n\n\n\n\n\n\n\n\nLatihan Numpy 1\n\n\n\n\n\n\ndata science\n\n\nexercise\n\n\npython\n\n\nnumpy\n\n\nbasic\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nsulthan a. karimov\n\n\n\n\n\n\n\n\n\n\n\n\nSolusi Tugas numpy 1\n\n\n\n\n\n\ndata science\n\n\nexercise\n\n\npython\n\n\nnumpy\n\n\nbasic\n\n\nsolution\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nsulthan a. karimov\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Untitled.html",
    "href": "posts/Data Science/Machine Learning/Untitled.html",
    "title": "Sulthan A. Karimov",
    "section": "",
    "text": "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n\nimg = load_img('datasets/images/food5k/evaluation/food/0.jpg', target_size=(150,150))\nimg = img_to_array(img)\nimg.shape\n\n(150, 150, 3)\n\n\n\nimport os\ndef input_target_split(train_dir,labels):\n    dataset = []\n    count = 0\n    for label in labels:\n        folder = os.path.join(train_dir,label)\n        for image in os.listdir(folder):\n            if os.path.isfile(os.path.join(folder,image)):\n                img=load_img(os.path.join(folder,image), target_size=(150,150))\n                img=img_to_array(img)\n                img=img/255.0\n                dataset.append((img,count))\n        print(f'\\rCompleted: {label}',end='')\n        count+=1\n    random.shuffle(dataset)\n    return dataset\n    #X, y = zip(*dataset)\n    \n    #return np.array(X),np.array(y)\n\ndataset = input_target_split('datasets/images/rockpaperscissors', ['rock', 'paper', 'scissors'])\n\nCompleted: scissors\n\n\n\nX,y = zip(*dataset)\ny\n\n(0,\n 2,\n 0,\n 2,\n 1,\n 0,\n 2,\n 1,\n 1,\n 2,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 1,\n 2,\n 2,\n 2,\n 0,\n 1,\n 2,\n 0,\n 0,\n 0,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 2,\n 2,\n 2,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 0,\n 2,\n 2,\n 0,\n 1,\n 0,\n 0,\n 2,\n 1,\n 1,\n 0,\n 2,\n 0,\n 0,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 0,\n 0,\n 1,\n 2,\n 1,\n 2,\n 2,\n 0,\n 0,\n 0,\n 1,\n 1,\n 2,\n 0,\n 2,\n 2,\n 0,\n 1,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 0,\n 0,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 1,\n 2,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 2,\n 0,\n 2,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 2,\n 1,\n 2,\n 0,\n 0,\n 2,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 2,\n 1,\n 1,\n 0,\n 1,\n 1,\n 2,\n 2,\n 0,\n 0,\n 1,\n 0,\n 2,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 2,\n 0,\n 2,\n 0,\n 1,\n 0,\n 0,\n 0,\n 2,\n 2,\n 2,\n 2,\n 0,\n 0,\n 0,\n 0,\n 2,\n 1,\n 2,\n 0,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 1,\n 2,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 2,\n 2,\n 2,\n 2,\n 2,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 2,\n 2,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 2,\n 1,\n 0,\n 1,\n 2,\n 0,\n 0,\n 2,\n 1,\n 2,\n 0,\n 1,\n 2,\n 1,\n 0,\n 1,\n 0,\n 0,\n 2,\n 1,\n 0,\n 2,\n 1,\n 2,\n 2,\n 2,\n 2,\n 0,\n 1,\n 1,\n 2,\n 0,\n 1,\n 2,\n 0,\n 1,\n 2,\n 2,\n 2,\n 0,\n 0,\n 0,\n 0,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 0,\n 0,\n 1,\n 2,\n 2,\n 2,\n 1,\n 0,\n 1,\n 1,\n 2,\n 2,\n 0,\n 0,\n 2,\n 0,\n 0,\n 0,\n 0,\n 1,\n 2,\n 1,\n 1,\n 2,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 2,\n 1,\n 2,\n 2,\n 0,\n 0,\n 2,\n 2,\n 1,\n 2,\n 1,\n 2,\n 0,\n 2,\n 2,\n 2,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 0,\n 0,\n 2,\n 2,\n 0,\n 1,\n 1,\n 0,\n 0,\n 2,\n 2,\n 1,\n 1,\n 0,\n 0,\n 0,\n 2,\n 2,\n 0,\n 2,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 2,\n 1,\n 0,\n 0,\n 2,\n 1,\n 0,\n 1,\n 2,\n 1,\n 1,\n 2,\n 1,\n 2,\n 0,\n 2,\n 0,\n 2,\n 2,\n 1,\n 0,\n 2,\n 1,\n 1,\n 1,\n 2,\n 0,\n 1,\n 1,\n 0,\n 2,\n 1,\n 1,\n 2,\n 1,\n 2,\n 1,\n 1,\n 1,\n 2,\n 0,\n 0,\n 1,\n 2,\n 2,\n 0,\n 0,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 2,\n 0,\n 1,\n 0,\n 0,\n 1,\n 2,\n 1,\n 2,\n 2,\n 2,\n 1,\n 2,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 2,\n 0,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 2,\n 0,\n 0,\n 1,\n 0,\n 0,\n 2,\n 0,\n 1,\n 1,\n 0,\n 2,\n 2,\n 2,\n 0,\n 0,\n 2,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 2,\n 0,\n 2,\n 0,\n 2,\n 0,\n 2,\n 2,\n 1,\n 0,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 0,\n 2,\n 0,\n 0,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 1,\n 1,\n 2,\n 2,\n 0,\n 1,\n 2,\n 0,\n 2,\n 0,\n 0,\n 2,\n 1,\n 2,\n 0,\n 0,\n 2,\n 2,\n 0,\n 2,\n 0,\n 0,\n 0,\n 1,\n 0,\n 2,\n 1,\n 1,\n 2,\n 0,\n 1,\n 2,\n 0,\n 2,\n 0,\n 2,\n 0,\n 1,\n 0,\n 0,\n 2,\n 0,\n 2,\n 1,\n 2,\n 0,\n 0,\n 0,\n 2,\n 2,\n 2,\n 1,\n 2,\n 1,\n 2,\n 0,\n 1,\n 0,\n 1,\n 2,\n 1,\n 0,\n 2,\n 0,\n 2,\n 2,\n 0,\n 2,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 2,\n 0,\n 1,\n 2,\n 2,\n 0,\n 2,\n 2,\n 1,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 0,\n 0,\n 2,\n 2,\n 0,\n 0,\n 0,\n 2,\n 1,\n 2,\n 1,\n 0,\n 0,\n 1,\n 2,\n 1,\n 2,\n 2,\n 1,\n 0,\n 1,\n 1,\n 2,\n 1,\n 2,\n 0,\n 2,\n 2,\n 1,\n 0,\n 0,\n 2,\n 0,\n 1,\n 2,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 0,\n 2,\n 0,\n 1,\n 0,\n 1,\n 1,\n 2,\n 1,\n 1,\n 0,\n 0,\n 2,\n 0,\n 0,\n 2,\n 0,\n 0,\n 1,\n 1,\n 2,\n 1,\n 0,\n 0,\n 0,\n 1,\n 2,\n 0,\n 2,\n 2,\n 2,\n 0,\n 0,\n 2,\n 2,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 2,\n 1,\n 2,\n 0,\n 2,\n 0,\n 0,\n 0,\n 1,\n 2,\n 0,\n 0,\n 0,\n 1,\n 2,\n 1,\n 2,\n 1,\n 2,\n 1,\n 0,\n 0,\n 1,\n 2,\n 0,\n 1,\n 0,\n 2,\n 0,\n 0,\n 2,\n 2,\n 2,\n 1,\n 0,\n 2,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 2,\n 1,\n 1,\n 2,\n 0,\n 2,\n 0,\n 2,\n 2,\n 0,\n 0,\n 2,\n 2,\n 2,\n 0,\n 0,\n 0,\n 2,\n 2,\n 1,\n 0,\n 2,\n 2,\n 0,\n 2,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 2,\n 2,\n 2,\n 1,\n 1,\n 2,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 2,\n 1,\n 2,\n 2,\n 1,\n 2,\n 0,\n 2,\n 0,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 2,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 2,\n 0,\n 2,\n 0,\n 1,\n 1,\n 0,\n 1,\n 2,\n 0,\n 0,\n 2,\n 2,\n 2,\n 2,\n 2,\n 1,\n 0,\n 2,\n 0,\n 1,\n 2,\n 1,\n 2,\n 2,\n 0,\n 0,\n 0,\n 0,\n 0,\n 2,\n 0,\n 1,\n 2,\n 1,\n 2,\n 0,\n 2,\n 1,\n 2,\n 1,\n 0,\n 2,\n 2,\n 1,\n 2,\n 0,\n 1,\n 2,\n 0,\n 2,\n 1,\n 0,\n 1,\n 0,\n 2,\n 2,\n 1,\n 2,\n 2,\n 0,\n 1,\n 2,\n 2,\n 0,\n 0,\n 1,\n 0,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 2,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 2,\n 0,\n 2,\n 0,\n 2,\n 2,\n 2,\n 2,\n 0,\n 2,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 0,\n 2,\n 2,\n 2,\n 2,\n 1,\n 0,\n 2,\n 0,\n 1,\n 1,\n 1,\n 2,\n 1,\n 2,\n 2,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 2,\n 1,\n 2,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 2,\n 1,\n 2,\n 1,\n 0,\n 2,\n 2,\n 2,\n 1,\n 0,\n 0,\n 2,\n 1,\n 1,\n 0,\n 2,\n 0,\n 1,\n 0,\n 2,\n 2,\n 1,\n 2,\n 0,\n 0,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 0,\n 2,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 2,\n 0,\n ...)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Untitled"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Transfer Learning.html",
    "href": "posts/Data Science/Machine Learning/Transfer Learning.html",
    "title": "import the necessary packages",
    "section": "",
    "text": "# import the necessary packages\nimport os\n# initialize the path to the *original* input directory of images\nORIG_INPUT_DATASET = \"Food-5K\"\n# initialize the base path to the *new* directory that will contain\n# our images after computing the training and testing split\nBASE_PATH = \"datasets/images/rps\"\n# define the names of the training, testing, and validation\n# directories\nTRAIN = \"train\"\nTEST = \"val\"\n# initialize the list of class label names\nCLASSES = [\"rock\", \"paper\", \"scissors\"]\n# set the batch size\nBATCH_SIZE = 4\n# initialize the label encoder file path and the output directory to\n# where the extracted features (in CSV file format) will be stored\nLE_PATH = os.path.sep.join([\"rpsoutput\", \"le.cpickle\"])\nBASE_CSV_PATH = \"rpsoutput\"\n# set the path to the serialized model after training\nMODEL_PATH = os.path.sep.join([\"rpsoutput\", \"model.cpickle\"])\n\n\nimport the necessary packages\nfrom pyimagesearch import config from imutils import paths import shutil import os # loop over the data splits for split in (config.TRAIN, config.TEST, config.VAL): # grab all image paths in the current split print(“[INFO] processing ‘{} split’…”.format(split)) p = os.path.sep.join([config.ORIG_INPUT_DATASET, split]) imagePaths = list(paths.list_images(p)) # loop over the image paths for imagePath in imagePaths: # extract class label from the filename filename = imagePath.split(os.path.sep)[-1] label = config.CLASSES[int(filename.split(“_“)[0])] # construct the path to the output directory dirPath = os.path.sep.join([config.BASE_PATH, split, label]) # if the output directory does not exist, create it if not os.path.exists(dirPath): os.makedirs(dirPath) # construct the path to the output image file and copy it p = os.path.sep.join([dirPath, filename]) shutil.copy2(imagePath, p)\n\n# import the necessary packages\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom imutils import paths\nimport numpy as np\nimport pickle\nimport random\nimport os\n# load the VGG16 network and initialize the label encoder\nprint(\"[INFO] loading network...\")\nmodel = VGG16(weights=\"imagenet\", include_top=False)\nle = None\n# loop over the data splits\nfor split in (TRAIN, TEST):\n    # grab all image paths in the current split\n    print(\"[INFO] processing '{} split'...\".format(split))\n    p = os.path.sep.join([BASE_PATH, split])\n    imagePaths = list(paths.list_images(p))\n    # randomly shuffle the image paths and then extract the class\n    # labels from the file paths\n    #random.shuffle(imagePaths)\n    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n    # if the label encoder is None, create it\n    if le is None:\n        le = LabelEncoder()\n        le.fit(labels)\n    # open the output CSV file for writing\n    csvPath = os.path.sep.join([BASE_CSV_PATH,\n        \"{}.csv\".format(split)])\n    csv = open(csvPath, \"w\")\n    # loop over the images in batches\n    for (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n        # extract the batch of images and labels, then initialize the\n        # list of actual images that will be passed through the network\n        # for feature extraction\n        print(\"[INFO] processing batch {}/{}\".format(b + 1,\n            int(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n        batchPaths = imagePaths[i:i + BATCH_SIZE]\n        batchLabels = le.transform(labels[i:i + BATCH_SIZE])\n        batchImages = []\n        # loop over the images and labels in the current batch\n        for imagePath in batchPaths:\n            # load the input image using the Keras helper utility\n            # while ensuring the image is resized to 224x224 pixels\n            image = load_img(imagePath, target_size=(224, 224))\n            image = img_to_array(image)\n            # preprocess the image by (1) expanding the dimensions and\n            # (2) subtracting the mean RGB pixel intensity from the\n            # ImageNet dataset\n            image = np.expand_dims(image, axis=0)\n            image = preprocess_input(image)\n            # add the image to the batch\n            batchImages.append(image)\n        # pass the images through the network and use the outputs as\n        # our actual features, then reshape the features into a\n        # flattened volume\n        batchImages = np.vstack(batchImages)\n        features = model.predict(batchImages, batch_size=BATCH_SIZE)\n        features = features.reshape((features.shape[0], 7 * 7 * 512))\n        # loop over the class labels and extracted features\n        for (label, vec) in zip(batchLabels, features):\n            # construct a row that exists of the class label and\n            # extracted features\n            vec = \",\".join([str(v) for v in vec])\n            csv.write(\"{},{}\\n\".format(label, vec))\n    # close the CSV file csv.close() # serialize the label encoder to disk f = open(LE_PATH, \"wb\")\nf.write(pickle.dumps(le))\nf.close()\n\n2024-06-12 13:09:00.027397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-06-12 13:09:01.839505: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.847179: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.847420: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.848740: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.848957: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.849144: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.893914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.894166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.894367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 13:09:01.894523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3503 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718197742.641173      90 service.cc:145] XLA service 0x7871e8004be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1718197742.641208      90 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\n2024-06-12 13:09:02.653166: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-06-12 13:09:02.746601: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\nI0000 00:00:1718197749.379902      90 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n\n[INFO] loading network...\n[INFO] processing 'train split'...\n[INFO] processing batch 1/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 7s 7s/step\n[INFO] processing batch 2/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step\n[INFO] processing batch 3/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n[INFO] processing batch 4/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 5/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n[INFO] processing batch 6/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n[INFO] processing batch 7/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\n[INFO] processing batch 8/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 9/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n[INFO] processing batch 10/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 11/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step\n[INFO] processing batch 12/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step\n[INFO] processing batch 13/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step\n[INFO] processing batch 14/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n[INFO] processing batch 15/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step\n[INFO] processing batch 16/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\n[INFO] processing batch 17/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\n[INFO] processing batch 18/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\n[INFO] processing batch 19/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step\n[INFO] processing batch 20/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 21/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\n[INFO] processing batch 22/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n[INFO] processing batch 23/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\n[INFO] processing batch 24/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n[INFO] processing batch 25/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\n[INFO] processing batch 26/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step\n[INFO] processing batch 27/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n[INFO] processing batch 28/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\n[INFO] processing batch 29/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 30/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step\n[INFO] processing batch 31/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step\n[INFO] processing batch 32/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n[INFO] processing batch 33/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n[INFO] processing batch 34/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step\n[INFO] processing batch 35/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 36/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 37/328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\n[INFO] processing batch 38/328\n\n\nKeyboardInterrupt: \n\n\n\n# import the necessary packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport pickle\nimport os\ndef load_data_split(splitPath):\n    # initialize the data and labels\n    data = []\n    labels = []\n    # loop over the rows in the data split file\n    for row in open(splitPath):\n        # extract the class label and features from the row\n        row = row.strip().split(\",\")\n        label = row[0]\n        features = np.array(row[1:], dtype=\"float\")\n        # update the data and label lists\n        data.append(features)\n        labels.append(label)\n    # convert the data and labels to NumPy arrays\n    data = np.array(data)\n    labels = np.array(labels)\n    # return a tuple of the data and labels\n    return (data, labels)\n\n# derive the paths to the training and testing CSV files\ntrainingPath = os.path.sep.join([BASE_CSV_PATH,\n    \"{}.csv\".format(TRAIN)])\ntestingPath = os.path.sep.join([BASE_CSV_PATH,\n    \"{}.csv\".format(TEST)])\n# load the data from disk\nprint(\"[INFO] loading data...\")\n(trainX, trainY) = load_data_split(trainingPath)\nprint(trainX.shape)\n(testX, testY) = load_data_split(testingPath)\n# load the label encoder from disk\nle = pickle.loads(open(LE_PATH, \"rb\").read())\n\n# train the model\nprint(\"[INFO] training model...\")\nprint(testX)\nmodelLogReg = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\",\n    max_iter=150)\nmodelLogReg.fit(trainX, trainY)\n# evaluate the model\nprint(\"[INFO] evaluating...\")\npreds = modelLogReg.predict(testX)\nprint(classification_report(testY, preds, target_names=le.classes_))\n# serialize the model to disk\nprint(\"[INFO] saving model...\")\nf = open(MODEL_PATH, \"wb\")\nf.write(pickle.dumps(modelLogReg))\nf.close()\n\n\nimport os\nimport numpy as np\n#from google.colab import files\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\ntestdir = 'datasets/images/rps/test/'\nuploaded = os.listdir(testdir)\nfig = plt.figure(figsize= (10, 10))\n#fig.tight_layout(pad=5.0)\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nfor i in range(len(uploaded)):\n    path = testdir + uploaded[i]\n    img = image.load_img(path, target_size = (224,224))\n\n    ax = fig.add_subplot(4, 4, i+1)\n    ax.imshow(img)\n    images = img_to_array(img)\n    images = np.expand_dims(images, axis=0)\n    images = preprocess_input(images)\n    #images = np.vstack([x])\n    features = model.predict(images, batch_size=BATCH_SIZE)\n    features = features.reshape((features.shape[0], 7 * 7 * 512))\n    pred = modelLogReg.predict(features)\n    ax.title.set_text(le.classes_[int(pred[0])])\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "import the necessary packages"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Submission-colab.html",
    "href": "posts/Data Science/Machine Learning/Submission-colab.html",
    "title": "Student Name : Sulthan A. Karimov",
    "section": "",
    "text": "Student Name : Sulthan A. Karimov\n\n\nUsername : sulthankarimov\n\n\nEmail : sulthankarimov@gmail.com\n\n!pip install split-folders[full] matplotlib jupyter_http_over_ws imutils\n\nRequirement already satisfied: split-folders[full] in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: jupyter_http_over_ws in /usr/local/lib/python3.10/dist-packages (0.0.8)\nRequirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from split-folders[full]) (4.66.4)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: notebook&gt;=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (6.5.5)\nRequirement already satisfied: six&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: tornado&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (6.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.1.4)\nRequirement already satisfied: pyzmq&lt;25,&gt;=17 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (24.0.1)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.1.0)\nRequirement already satisfied: traitlets&gt;=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.7.1)\nRequirement already satisfied: jupyter-core&gt;=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.7.2)\nRequirement already satisfied: jupyter-client&lt;8,&gt;=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.1.12)\nRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.0)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.10.4)\nRequirement already satisfied: nbconvert&gt;=5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.5.4)\nRequirement already satisfied: nest-asyncio&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.6.0)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.5.6)\nRequirement already satisfied: Send2Trash&gt;=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.3)\nRequirement already satisfied: terminado&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.1)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.20.0)\nRequirement already satisfied: nbclassic&gt;=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.1.0)\nRequirement already satisfied: platformdirs&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core&gt;=4.6.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.2.2)\nRequirement already satisfied: notebook-shim&gt;=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.4)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.9.4)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.12.3)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.1.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.1)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.1.5)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.8.4)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.10.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.5.1)\nRequirement already satisfied: pygments&gt;=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.16.1)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.0)\nRequirement already satisfied: fastjsonschema&gt;=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.19.1)\nRequirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.19.2)\nRequirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado&gt;=0.8.3-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (21.2.0)\nRequirement already satisfied: ipython&gt;=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (7.34.0)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (67.7.2)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.19.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.0.47)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.7)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.9.0)\nRequirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.35.1)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.1)\nRequirement already satisfied: jupyter-server&lt;3,&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.24.0)\nRequirement already satisfied: cffi&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.5)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-&gt;nbconvert&gt;=5-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.5.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.22)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.8.4)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server&lt;3,&gt;=1.8-&gt;notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.7.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server&lt;3,&gt;=1.8-&gt;notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.13)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=1.8-&gt;notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.7)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=1.8-&gt;notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=1.8-&gt;notebook-shim&gt;=0.2.3-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.2.1)\n\n\n\n# import all needed libraries\nimport zipfile, os, shutil, splitfolders, re, random\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom imutils import paths\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import classification_report\n\n\n# defining directories\nlocal_dir = '/tmp/'\ndataset_name = 'rockpaperscissors'\nlocal_data = local_dir + dataset_name\nlocal_zip = local_data + '.zip'\n\n\n# dowload dataset (pass if exist)\n!test -f $local_zip || wget --no-check-certificate \\\n https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \\\n -O $local_zip\n\n\n# extract dataset\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall(local_dir)\nzip_ref.close()\n\n\n# prepare train & validation split\nshutil.rmtree(local_data + '/rps-cv-images')\nos.remove(local_data + '/README_rpc-cv-images.txt')\nprint(os.listdir(local_data))\n\nbase_dir = local_dir + '/rps'\nif os.path.exists(base_dir) == True:\n  shutil.rmtree(base_dir)\n\nsplitfolders.ratio(local_data, base_dir, ratio=(.6,.4))\ntrain_dir = os.path.join(base_dir, 'train')\nprint('amount of training sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/train')))\nvalidation_dir = os.path.join(base_dir, 'val')\nprint('amount of validation sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/val')))\n\n['rock', 'scissors', 'paper']\namount of training sample :  1312\namount of validation sample :  876\n\n\nCopying files: 2188 files [00:01, 2052.05 files/s]\n\n\n\n# prepare test folder for classification report\nfiles_list = []\n\nfor root, dirs, files in os.walk(local_data):\n  for dir in dirs:\n    results = os.walk(local_data + '/' + dir)\n    for result in results:\n      cat_list = []\n      for file in result[2]:\n        if file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"):\n          cat_list.append(os.path.join(root + '/' + dir, file))\n    files_list.append(cat_list)\n\nfor imgs in files_list:\n  category = re.findall('(?&lt;=rs\\/).*?(?=\\/)', imgs[0])[0]\n  filesToCopy = random.sample(imgs, 4)\n  destPath = base_dir + '/test/' + str(category)\n  if os.path.isdir(destPath) == False:\n    os.makedirs(destPath)\n  for file in filesToCopy:\n    shutil.copy(file, destPath)\n\ntest_dir = os.path.join(base_dir, 'test')\n\n\n# directory summary\nprint(os.listdir(train_dir))\nprint(os.listdir(validation_dir))\nprint(os.listdir(test_dir))\n\n['rock', 'scissors', 'paper']\n['rock', 'scissors', 'paper']\n['rock', 'scissors', 'paper']\n\n\n\n# preparing generator\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n    rotation_range = 20,\n    horizontal_flip = True,\n    shear_range = 0.2,\n    fill_mode = 'nearest',\n    preprocessing_function = preprocess_input,\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale = 1./255)\n\n\n# defining rgb mean for every generator\nmean = np.array([123.68, 116.779, 103.939], dtype = 'float32')\ntrain_datagen.mean = mean\ntest_datagen.mean = mean\n\n\n# flow data to generator\nBATCH_SIZE = 32\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\ntotalTrain = len(list(paths.list_images(train_dir)))\ntotalVal = len(list(paths.list_images(validation_dir)))\ntotalTest = len(list(paths.list_images(test_dir)))\n\nFound 1312 images belonging to 3 classes.\nFound 876 images belonging to 3 classes.\nFound 12 images belonging to 3 classes.\n\n\n\n# preparing baseModel\nbaseModel = VGG16(weights=\"imagenet\", include_top=False,\n    input_tensor=Input(shape=(224, 224, 3)))\nheadModel = baseModel.output\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(512, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(3, activation=\"softmax\")(headModel)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n\n\n\n# freeze hidden layers to preserve model features\nfor layer in baseModel.layers:\n    layer.trainable = False\n\n\n# prepare callbacks\nEarlyStop = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    restore_best_weights = True,\n    min_delta = 0.1\n)\n\nModelCP = ReduceLROnPlateau(\n    monitor = 'val_loss',\n    factor = 0.5,\n    patience = 1,\n    verbose = 1\n)\n\ncallbacks = [EarlyStop, ModelCP]\n\n\n# compile the model with frozen layers\nprint(\"[INFO] compiling model...\")\nopt = SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\nprint(\"[INFO] training head...\")\nH = model.fit(\n    x=train_generator,\n    steps_per_epoch=totalTrain // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=totalVal // BATCH_SIZE,\n    epochs=25,\n    callbacks=callbacks,\n)\n\n[INFO] compiling model...\n[INFO] training head...\nEpoch 1/25\n41/41 [==============================] - 38s 606ms/step - loss: 1.6104 - accuracy: 0.3948 - val_loss: 1.1712 - val_accuracy: 0.3299 - lr: 1.0000e-04\nEpoch 2/25\n41/41 [==============================] - 24s 587ms/step - loss: 1.1336 - accuracy: 0.4223 - val_loss: 0.9012 - val_accuracy: 0.5764 - lr: 1.0000e-04\nEpoch 3/25\n41/41 [==============================] - ETA: 0s - loss: 0.9340 - accuracy: 0.5777\nEpoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n41/41 [==============================] - 22s 532ms/step - loss: 0.9340 - accuracy: 0.5777 - val_loss: 1.0275 - val_accuracy: 0.3738 - lr: 1.0000e-04\nEpoch 4/25\n41/41 [==============================] - 24s 596ms/step - loss: 0.8847 - accuracy: 0.5998 - val_loss: 0.7451 - val_accuracy: 0.7083 - lr: 5.0000e-05\nEpoch 5/25\n41/41 [==============================] - 22s 541ms/step - loss: 0.7422 - accuracy: 0.7919 - val_loss: 0.6416 - val_accuracy: 0.9375 - lr: 5.0000e-05\nEpoch 6/25\n41/41 [==============================] - 21s 520ms/step - loss: 0.6953 - accuracy: 0.7698 - val_loss: 0.5985 - val_accuracy: 0.9201 - lr: 5.0000e-05\nEpoch 7/25\n41/41 [==============================] - ETA: 0s - loss: 0.6743 - accuracy: 0.7790\nEpoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n41/41 [==============================] - 22s 532ms/step - loss: 0.6743 - accuracy: 0.7790 - val_loss: 0.6319 - val_accuracy: 0.8194 - lr: 5.0000e-05\nEpoch 8/25\n41/41 [==============================] - 23s 565ms/step - loss: 0.7432 - accuracy: 0.7096 - val_loss: 0.5822 - val_accuracy: 0.8646 - lr: 2.5000e-05\nEpoch 9/25\n41/41 [==============================] - 21s 518ms/step - loss: 0.6423 - accuracy: 0.8056 - val_loss: 0.5275 - val_accuracy: 0.9225 - lr: 2.5000e-05\nEpoch 10/25\n41/41 [==============================] - 22s 534ms/step - loss: 0.5945 - accuracy: 0.8697 - val_loss: 0.5073 - val_accuracy: 0.9363 - lr: 2.5000e-05\nEpoch 11/25\n41/41 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8933\nEpoch 11: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n41/41 [==============================] - 24s 589ms/step - loss: 0.5495 - accuracy: 0.8933 - val_loss: 0.5212 - val_accuracy: 0.9271 - lr: 2.5000e-05\nEpoch 12/25\n41/41 [==============================] - 22s 537ms/step - loss: 0.5244 - accuracy: 0.9002 - val_loss: 0.4796 - val_accuracy: 0.9340 - lr: 1.2500e-05\nEpoch 13/25\n41/41 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.8941Restoring model weights from the end of the best epoch: 9.\n\nEpoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n41/41 [==============================] - 21s 522ms/step - loss: 0.5346 - accuracy: 0.8941 - val_loss: 0.4812 - val_accuracy: 0.9213 - lr: 1.2500e-05\nEpoch 13: early stopping\n\n\n\n# create classification_report for trained model\nprint(\"[INFO] evaluating after fine-tuning network head...\")\ntest_generator.reset()\npredIdxs = model.predict(x=test_generator,\n    steps=(totalTest // BATCH_SIZE) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(test_generator.classes, predIdxs,\n    target_names=test_generator.class_indices.keys()))\n\n[INFO] evaluating after fine-tuning network head...\n1/1 [==============================] - 3s 3s/step\n              precision    recall  f1-score   support\n\n       paper       1.00      1.00      1.00         4\n        rock       1.00      1.00      1.00         4\n    scissors       1.00      1.00      1.00         4\n\n    accuracy                           1.00        12\n   macro avg       1.00      1.00      1.00        12\nweighted avg       1.00      1.00      1.00        12\n\n\n\n\n# recreate model with unfrozen layers\nclear_session()\ntrain_generator.reset()\nvalidation_generator.reset()\nfor layer in baseModel.layers[15:]:\n    layer.trainable = True\n\n\n# recompile and retrain the model after unfreezing layers\nprint(\"[INFO] re-compiling model...\")\nopt = SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\nH = model.fit(\n    x=train_generator,\n    steps_per_epoch=totalTrain // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=totalVal // BATCH_SIZE,\n    epochs=20,\n    callbacks=callbacks,\n)\n\n[INFO] re-compiling model...\nEpoch 1/20\n41/41 [==============================] - 26s 562ms/step - loss: 0.5498 - accuracy: 0.8255 - val_loss: 0.5427 - val_accuracy: 0.7882 - lr: 1.0000e-04\nEpoch 2/20\n41/41 [==============================] - 22s 535ms/step - loss: 0.3568 - accuracy: 0.9093 - val_loss: 0.2297 - val_accuracy: 0.9525 - lr: 1.0000e-04\nEpoch 3/20\n41/41 [==============================] - 21s 514ms/step - loss: 0.2275 - accuracy: 0.9459 - val_loss: 0.1615 - val_accuracy: 0.9618 - lr: 1.0000e-04\nEpoch 4/20\n41/41 [==============================] - 23s 557ms/step - loss: 0.1356 - accuracy: 0.9688 - val_loss: 0.1183 - val_accuracy: 0.9711 - lr: 1.0000e-04\nEpoch 5/20\n41/41 [==============================] - 24s 571ms/step - loss: 0.1202 - accuracy: 0.9726 - val_loss: 0.1065 - val_accuracy: 0.9803 - lr: 1.0000e-04\nEpoch 6/20\n41/41 [==============================] - 24s 576ms/step - loss: 0.1012 - accuracy: 0.9771 - val_loss: 0.0918 - val_accuracy: 0.9838 - lr: 1.0000e-04\nEpoch 7/20\n41/41 [==============================] - 22s 527ms/step - loss: 0.0926 - accuracy: 0.9787 - val_loss: 0.0674 - val_accuracy: 0.9873 - lr: 1.0000e-04\nEpoch 8/20\n41/41 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9870Restoring model weights from the end of the best epoch: 4.\n41/41 [==============================] - 23s 557ms/step - loss: 0.0648 - accuracy: 0.9870 - val_loss: 0.0622 - val_accuracy: 0.9873 - lr: 1.0000e-04\nEpoch 8: early stopping\n\n\n\n# create classification_report for full model\nprint(\"[INFO] evaluating after fine-tuning network...\")\ntest_generator.reset()\npredIdxs = model.predict(x=test_generator,\n    steps=(totalTest // BATCH_SIZE) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(test_generator.classes, predIdxs,\n    target_names=test_generator.class_indices.keys()))\n\n[INFO] evaluating after fine-tuning network...\n1/1 [==============================] - 0s 188ms/step\n              precision    recall  f1-score   support\n\n       paper       1.00      1.00      1.00         4\n        rock       1.00      1.00      1.00         4\n    scissors       1.00      1.00      1.00         4\n\n    accuracy                           1.00        12\n   macro avg       1.00      1.00      1.00        12\nweighted avg       1.00      1.00      1.00        12\n\n\n\n\nimport os\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\ntestdir = test_dir\nuploaded = list(paths.list_images(testdir))\nfig = plt.figure(figsize= (10, 10))\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1,\n                    right=0.9,\n                    top=0.9,\n                    wspace=0.4,\n                    hspace=0.4)\nfor i in range(len(uploaded)):\n    path = uploaded[i]\n    img = image.load_img(path, target_size = (224,224))\n\n    ax = fig.add_subplot(4, 4, i+1)\n    ax.imshow(img)\n    images = image.img_to_array(img)\n    images = np.expand_dims(images, axis=0)\n    images = preprocess_input(images)\n    pred = model.predict(images)\n    ax.title.set_text(list(train_generator.class_indices.keys())[np.argmax(pred, axis = 1)[0]])\n\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "**Student Name** : Sulthan A. Karimov"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/ML - Supervised Learning.html",
    "href": "posts/Data Science/Machine Learning/ML - Supervised Learning.html",
    "title": "Machine Learning - Supervised Learning",
    "section": "",
    "text": "One of Machine Learning models is Supervised Learning. as named, this model need human supervision to achieve it’s goal. if the goal is classifying bunch of objects to several categories, this model needs human to tell whether it’s category A, B, or C etc. given that labels the model try to find the best possible pattern of data to be determined as one of those categories.\nthere are several kind of Supervised Learning Model ML, one of those and the most simple is Classification. to know more about Classification check this out.",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Machine Learning - Supervised Learning"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#classification",
    "href": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#classification",
    "title": "Machine Learning - Supervised Learning",
    "section": "Classification",
    "text": "Classification\n\nimport pandas as pd\n\niris = pd.read_csv('datasets/iris/Iris.csv')\n\n\niris\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n146\n6.7\n3.0\n5.2\n2.3\nIris-virginica\n\n\n146\n147\n6.3\n2.5\n5.0\n1.9\nIris-virginica\n\n\n147\n148\n6.5\n3.0\n5.2\n2.0\nIris-virginica\n\n\n148\n149\n6.2\n3.4\n5.4\n2.3\nIris-virginica\n\n\n149\n150\n5.9\n3.0\n5.1\n1.8\nIris-virginica\n\n\n\n\n150 rows × 6 columns\n\n\n\n\niris.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             150 non-null    int64  \n 1   SepalLengthCm  150 non-null    float64\n 2   SepalWidthCm   150 non-null    float64\n 3   PetalLengthCm  150 non-null    float64\n 4   PetalWidthCm   150 non-null    float64\n 5   Species        150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\n\n\n\niris.head()\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n# sropping unneeded data\niris.drop('Id', axis=1, inplace=True)\n\n\nX = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny = iris['Species']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n\n\nfrom sklearn import tree\n\nclf = tree.DecisionTreeClassifier()\n\n\n# with cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(clf, X, y, cv=5)\n\n\nscores\n\narray([0.96666667, 0.96666667, 0.9       , 0.96666667, 1.        ])",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Machine Learning - Supervised Learning"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#linear-regression",
    "href": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#linear-regression",
    "title": "Machine Learning - Supervised Learning",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nimport numpy as np\n\n# make dummy data of rooms\nbedrooms = np.array([1,1,2,2,3,4,4,5,5,5])\n \n# make dummy price data in dolar\nhouse_price = np.array([15000, 18000, 27000, 34000, 50000, 68000, 65000, 81000,85000, 90000])\n\n\n# visualize in scatterplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nplt.scatter(bedrooms, house_price)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n \n# train the model with LinearRegression.fit()\nbedrooms = bedrooms.reshape(-1, 1)\nlinreg = LinearRegression()\nlinreg.fit(bedrooms, house_price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# plotting the corelation between number of rooms and house_prices\nplt.scatter(bedrooms, house_price)\nplt.plot(bedrooms, linreg.predict(bedrooms))",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Machine Learning - Supervised Learning"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#logistic-regression",
    "href": "posts/Data Science/Machine Learning/ML - Supervised Learning.html#logistic-regression",
    "title": "Machine Learning - Supervised Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport pandas as pd\n\ndf = pd.read_csv('datasets/socmedAds/Social_Network_Ads.csv')\ndf\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n395\n15691863\nFemale\n46\n41000\n1\n\n\n396\n15706071\nMale\n51\n23000\n1\n\n\n397\n15654296\nFemale\n50\n20000\n1\n\n\n398\n15755018\nMale\n36\n33000\n0\n\n\n399\n15594041\nFemale\n49\n36000\n1\n\n\n\n\n400 rows × 5 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 400 entries, 0 to 399\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   User ID          400 non-null    int64 \n 1   Gender           400 non-null    object\n 2   Age              400 non-null    int64 \n 3   EstimatedSalary  400 non-null    int64 \n 4   Purchased        400 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 15.8+ KB\n\n\n\ndata = df.drop(columns=['User ID'])\n\ndata = pd.get_dummies(data)\ndata\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\nGender_Female\nGender_Male\n\n\n\n\n0\n19\n19000\n0\nFalse\nTrue\n\n\n1\n35\n20000\n0\nFalse\nTrue\n\n\n2\n26\n43000\n0\nTrue\nFalse\n\n\n3\n27\n57000\n0\nTrue\nFalse\n\n\n4\n19\n76000\n0\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n\n\n395\n46\n41000\n1\nTrue\nFalse\n\n\n396\n51\n23000\n1\nFalse\nTrue\n\n\n397\n50\n20000\n1\nTrue\nFalse\n\n\n398\n36\n33000\n0\nFalse\nTrue\n\n\n399\n49\n36000\n1\nTrue\nFalse\n\n\n\n\n400 rows × 5 columns\n\n\n\n\nX = data[['Age', 'EstimatedSalary', 'Gender_Female', 'Gender_Male']]\ny = data['Purchased']\n\n\n# data normalization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calculating the mean and standard deviation of every attribute column\n# to be used on every transform function\nscaler.fit(X)\nscaled_data = scaler.transform(X)\nscaled_data = pd.DataFrame(scaled_data, columns=X.columns)\nscaled_data\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nGender_Female\nGender_Male\n\n\n\n\n0\n-1.781797\n-1.490046\n-1.020204\n1.020204\n\n\n1\n-0.253587\n-1.460681\n-1.020204\n1.020204\n\n\n2\n-1.113206\n-0.785290\n0.980196\n-0.980196\n\n\n3\n-1.017692\n-0.374182\n0.980196\n-0.980196\n\n\n4\n-1.781797\n0.183751\n-1.020204\n1.020204\n\n\n...\n...\n...\n...\n...\n\n\n395\n0.797057\n-0.844019\n0.980196\n-0.980196\n\n\n396\n1.274623\n-1.372587\n-1.020204\n1.020204\n\n\n397\n1.179110\n-1.460681\n0.980196\n-0.980196\n\n\n398\n-0.158074\n-1.078938\n-1.020204\n1.020204\n\n\n399\n1.083596\n-0.990844\n0.980196\n-0.980196\n\n\n\n\n400 rows × 4 columns\n\n\n\n\n# validation with cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\n\nmodel = linear_model.LogisticRegression()\nscores = cross_val_score(model, scaled_data, y, cv=5)\n\n\nscores\n\narray([0.7   , 0.95  , 0.9375, 0.8125, 0.7   ])\n\n\n\nfrom sklearn.model_selection import train_test_split\n \nX_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.2, random_state=1)\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# examine model accuracy\nmodel.score(X_test, y_test)\n\n0.825",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Machine Learning - Supervised Learning"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Dimensionality Reduction.html",
    "href": "posts/Data Science/Machine Learning/Dimensionality Reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\nattributes = iris.data\nlabel = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    attributes, label, test_size=0.2, random_state=1)\n\n\n\n\nfrom sklearn import tree\n\ndt = tree.DecisionTreeClassifier()\nfirst = dt.fit(X_train, y_train)\nfirst.score(X_test, y_test)\n\n0.9666666666666667\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\n\npca_attributes = pca.fit_transform(X_train)\n\npca.explained_variance_ratio_\n\narray([0.92848323, 0.04764372, 0.01931005, 0.004563  ])\n\n\n\npca = PCA(n_components = 2)\n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.fit_transform(X_test)\n\n\nsecond = dt.fit(X_train_pca, y_train)\n\nsecond.score(X_test_pca, y_test)\n\n0.9333333333333333",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Dimensionality Reduction.html#pca",
    "href": "posts/Data Science/Machine Learning/Dimensionality Reduction.html#pca",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\nattributes = iris.data\nlabel = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    attributes, label, test_size=0.2, random_state=1)\n\n\n\n\nfrom sklearn import tree\n\ndt = tree.DecisionTreeClassifier()\nfirst = dt.fit(X_train, y_train)\nfirst.score(X_test, y_test)\n\n0.9666666666666667\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\n\npca_attributes = pca.fit_transform(X_train)\n\npca.explained_variance_ratio_\n\narray([0.92848323, 0.04764372, 0.01931005, 0.004563  ])\n\n\n\npca = PCA(n_components = 2)\n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.fit_transform(X_test)\n\n\nsecond = dt.fit(X_train_pca, y_train)\n\nsecond.score(X_test_pca, y_test)\n\n0.9333333333333333",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "posts/livekit/1_first_encounter_with_livekit.html",
    "href": "posts/livekit/1_first_encounter_with_livekit.html",
    "title": "my first encounter with webrtc and livekit agents",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Livekit",
      "my first encounter with webrtc and livekit agents"
    ]
  },
  {
    "objectID": "posts/Exercises/numpy task1.html",
    "href": "posts/Exercises/numpy task1.html",
    "title": "Latihan Numpy 1",
    "section": "",
    "text": "Perusahaan Anda bergerak di bidang retail pakaian dan ingin menganalisa data penjualan untuk berbagai kategori produk (contoh: kemeja, celana, sepatu) dan ukuran (S, M, L, XL) di berbagai lokasi (toko). Data penjualan disimpan dalam array NumPy bernama sales_data dengan struktur berikut (data dalam struktur hanya contoh):\n\nimport numpy as np\n\nsales_data = np.array([\n[[10, 35, 15, 20],  # Penjualan Lokasi 1\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [5,  35, 8, 12]],\n\n[[12, 35, 18, 15],  # Penjualan Lokasi 2\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [7,  35, 9, 11]]\n])\n\n# sales_data.shape = (jumlah_lokasi, jumlah_kategori, jumlah_ukuran)\n\nfile tugas bisa didownload [di sini](https://github.com/sakarimov/my-personal-blog/raw/main/mybook/notes/numpy%231.txt)\n\n\n\n\n\n\n\nNote\n\n\n\nuntuk membuka file ini ikuti petunjuk berikut:\nimport numpy as np\n\nload_file = np.loadtxt('path/to/the/file/you/download')\nload_original = load_file.reshape(4,7,4)\n\n\n\n\n\n\nIdentifikasi Kategori Penjualan Tertinggi menurut Lokasi (Slicing Lanjutan):\n\nGunakan teknik slicing tingkat lanjut dalam NumPy untuk menghitung total penjualan untuk setiap kategori di semua ukuran di setiap lokasi.\nPetunjuk: Anda dapat menggabungkan slicing dasar (:) untuk memilih lokasi tertentu dan masking boolean untuk menjumlahkan di sepanjang sumbu kategori.\nIdentifikasi kategori dengan penjualan tertinggi untuk setiap lokasi.\n\nBandingkan Distribusi Ukuran untuk Kategori Spesifik:\n\nFokus pada kategori tertentu (misalnya, kemeja) di semua lokasi.\nGunakan teknik slicing atau pengindeksan array untuk memilih hanya data penjualan kategori tersebut.\nHitung total penjualan untuk setiap ukuran (jumlahkan di sepanjang sumbu ukuran).\nAnalisa distribusi ukuran (misalnya, ukuran mana yang paling banyak terjual untuk kategori tersebut secara keseluruhan).\n\nTemukan Lokasi dengan Penjualan Rendah pada Ukuran Tertentu:\n\nTentukan ukuran tertentu (misalnya, XL) yang mungkin perlu diisi ulang stoknya.\nGunakan masking boolean untuk memilih hanya data penjualan ukuran tersebut dari seluruh array.\nHitung total penjualan produk dengan ukuran tersebut di setiap lokasi (jumlahkan di sepanjang semua sumbu lainnya).\nIdentifikasi lokasi dengan penjualan di bawah ambang batas tertentu untuk produk dengan ukuran tersebut, yang menunjukkan potensi kebutuhan untuk mengisi ulang stok.",
    "crumbs": [
      "About",
      "Posts",
      "Exercises",
      "Latihan Numpy 1"
    ]
  },
  {
    "objectID": "posts/Exercises/numpy task1.html#skenario",
    "href": "posts/Exercises/numpy task1.html#skenario",
    "title": "Latihan Numpy 1",
    "section": "",
    "text": "Perusahaan Anda bergerak di bidang retail pakaian dan ingin menganalisa data penjualan untuk berbagai kategori produk (contoh: kemeja, celana, sepatu) dan ukuran (S, M, L, XL) di berbagai lokasi (toko). Data penjualan disimpan dalam array NumPy bernama sales_data dengan struktur berikut (data dalam struktur hanya contoh):\n\nimport numpy as np\n\nsales_data = np.array([\n[[10, 35, 15, 20],  # Penjualan Lokasi 1\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [25, 35, 30, 18],\n [5,  35, 8, 12]],\n\n[[12, 35, 18, 15],  # Penjualan Lokasi 2\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [22, 35, 28, 20],\n [7,  35, 9, 11]]\n])\n\n# sales_data.shape = (jumlah_lokasi, jumlah_kategori, jumlah_ukuran)\n\nfile tugas bisa didownload [di sini](https://github.com/sakarimov/my-personal-blog/raw/main/mybook/notes/numpy%231.txt)\n\n\n\n\n\n\n\nNote\n\n\n\nuntuk membuka file ini ikuti petunjuk berikut:\nimport numpy as np\n\nload_file = np.loadtxt('path/to/the/file/you/download')\nload_original = load_file.reshape(4,7,4)",
    "crumbs": [
      "About",
      "Posts",
      "Exercises",
      "Latihan Numpy 1"
    ]
  },
  {
    "objectID": "posts/Exercises/numpy task1.html#tugas",
    "href": "posts/Exercises/numpy task1.html#tugas",
    "title": "Latihan Numpy 1",
    "section": "",
    "text": "Identifikasi Kategori Penjualan Tertinggi menurut Lokasi (Slicing Lanjutan):\n\nGunakan teknik slicing tingkat lanjut dalam NumPy untuk menghitung total penjualan untuk setiap kategori di semua ukuran di setiap lokasi.\nPetunjuk: Anda dapat menggabungkan slicing dasar (:) untuk memilih lokasi tertentu dan masking boolean untuk menjumlahkan di sepanjang sumbu kategori.\nIdentifikasi kategori dengan penjualan tertinggi untuk setiap lokasi.\n\nBandingkan Distribusi Ukuran untuk Kategori Spesifik:\n\nFokus pada kategori tertentu (misalnya, kemeja) di semua lokasi.\nGunakan teknik slicing atau pengindeksan array untuk memilih hanya data penjualan kategori tersebut.\nHitung total penjualan untuk setiap ukuran (jumlahkan di sepanjang sumbu ukuran).\nAnalisa distribusi ukuran (misalnya, ukuran mana yang paling banyak terjual untuk kategori tersebut secara keseluruhan).\n\nTemukan Lokasi dengan Penjualan Rendah pada Ukuran Tertentu:\n\nTentukan ukuran tertentu (misalnya, XL) yang mungkin perlu diisi ulang stoknya.\nGunakan masking boolean untuk memilih hanya data penjualan ukuran tersebut dari seluruh array.\nHitung total penjualan produk dengan ukuran tersebut di setiap lokasi (jumlahkan di sepanjang semua sumbu lainnya).\nIdentifikasi lokasi dengan penjualan di bawah ambang batas tertentu untuk produk dengan ukuran tersebut, yang menunjukkan potensi kebutuhan untuk mengisi ulang stok.",
    "crumbs": [
      "About",
      "Posts",
      "Exercises",
      "Latihan Numpy 1"
    ]
  },
  {
    "objectID": "posts/Exercises/numpy task1 solution.html",
    "href": "posts/Exercises/numpy task1 solution.html",
    "title": "Solusi Tugas numpy 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Exercises",
      "Solusi Tugas numpy 1"
    ]
  },
  {
    "objectID": "posts/opinions/salut dengan bangsa indonesia.html",
    "href": "posts/opinions/salut dengan bangsa indonesia.html",
    "title": "salut dengan bangsa indonesia",
    "section": "",
    "text": "Salut dengan Bangsa Ini\nsaya salut sekali dengan bangsa Indonesia dalam menyikapi masalah-masalah yang mencuat baru-baru ini. bagaimana tidak?, gempuran itu datang dari berbagai arah, ekonomi, sosial, budaya, bahkan politik. Bangsa Indonesia ini umurnya sudah tidak muda lagi, lebih tua dari negaranya sendiri. jadi maklum kalau masalah-masalah yang muncul belakangan tak menjadi persoalan sebesar kehancuran sebuah bangsa. paling hanya menjadi riak-riak kecil dalam mewarnai kehidupan bernegara. semoga kondisi ini terus jadi kekuatan untuk seluruh Bangsa Indonesia, sehingga masalah selevel negara tidak bisa mengusik kemakmuran hidup kita.\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Opinions",
      "salut dengan bangsa indonesia"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/JanKenPon CNN Feature Extraction.html",
    "href": "posts/Data Science/Machine Learning/JanKenPon CNN Feature Extraction.html",
    "title": "Student Name : Sulthan A. Karimov",
    "section": "",
    "text": "Student Name : Sulthan A. Karimov\n\n\nUsername : sulthankarimov\n\n\nEmail : sulthankarimov@gmail.com\n\n!pip install split-folders[full] matplotlib jupyter_http_over_ws\n\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.3)\nRequirement already satisfied: jupyter_http_over_ws in /usr/local/lib/python3.11/dist-packages (0.0.8)\nRequirement already satisfied: split-folders[full] in /usr/local/lib/python3.11/dist-packages (0.5.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from split-folders[full]) (4.66.4)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: notebook&gt;=5.0 in /usr/local/lib/python3.11/dist-packages (from jupyter_http_over_ws) (7.1.1)\nRequirement already satisfied: six&gt;=1.6.0 in /usr/lib/python3/dist-packages (from jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: tornado&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from jupyter_http_over_ws) (6.4)\nRequirement already satisfied: jupyter-server&lt;3,&gt;=2.4.0 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.13.0)\nRequirement already satisfied: jupyterlab-server&lt;3,&gt;=2.22.1 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.25.3)\nRequirement already satisfied: jupyterlab&lt;4.2,&gt;=4.1.1 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.1.4)\nRequirement already satisfied: notebook-shim&lt;0.3,&gt;=0.2 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.4)\nRequirement already satisfied: anyio&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.3.0)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.1.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.1.3)\nRequirement already satisfied: jupyter-client&gt;=7.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (8.6.0)\nRequirement already satisfied: jupyter-core!=5.0.*,&gt;=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.7.1)\nRequirement already satisfied: jupyter-events&gt;=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.5.2)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (7.16.2)\nRequirement already satisfied: nbformat&gt;=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.9.2)\nRequirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (7.7.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.20.0)\nRequirement already satisfied: pyzmq&gt;=24 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (25.1.2)\nRequirement already satisfied: send2trash&gt;=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.2)\nRequirement already satisfied: terminado&gt;=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.0)\nRequirement already satisfied: traitlets&gt;=5.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.14.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.7.0)\nRequirement already satisfied: async-lru&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.4)\nRequirement already satisfied: httpx&gt;=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.27.0)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.29.3)\nRequirement already satisfied: jupyter-lsp&gt;=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.2.4)\nRequirement already satisfied: babel&gt;=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.14.0)\nRequirement already satisfied: json5&gt;=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.22)\nRequirement already satisfied: jsonschema&gt;=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.21.1)\nRequirement already satisfied: requests&gt;=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.31.0)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.6)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.0.4)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*-&gt;httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.14.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.1.5)\nRequirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.33.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.0)\nRequirement already satisfied: platformdirs&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,&gt;=4.12-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.2.0)\nRequirement already satisfied: python-json-logger&gt;=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.7)\nRequirement already satisfied: pyyaml&gt;=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.0.1)\nRequirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.4)\nRequirement already satisfied: rfc3986-validator&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.12.3)\nRequirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.1.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.1)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.3.0)\nRequirement already satisfied: mistune&lt;4,&gt;=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.0.2)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.5.1)\nRequirement already satisfied: pygments&gt;=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.17.2)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.2.1)\nRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.11/dist-packages (from nbformat&gt;=5.3.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.19.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.31-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.31-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.2.1)\nRequirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado&gt;=0.8.3-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (21.2.0)\nRequirement already satisfied: comm&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.1)\nRequirement already satisfied: debugpy&gt;=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.1)\nRequirement already satisfied: ipython&gt;=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (8.22.2)\nRequirement already satisfied: matplotlib-inline&gt;=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.6)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.9.8)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.5.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.19.1)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.0.43)\nRequirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.6.3)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.9.0)\nRequirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.5.1)\nRequirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (20.11.0)\nRequirement already satisfied: jsonpointer&gt;1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.4)\nRequirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.0)\nRequirement already satisfied: webcolors&gt;=1.11 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.13)\nRequirement already satisfied: cffi&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.5)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.21)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi&gt;=0.16-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.8.3)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.13)\nRequirement already satisfied: arrow&gt;=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.0)\nRequirement already satisfied: executing&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.1)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.4.1)\nRequirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.2)\nRequirement already satisfied: types-python-dateutil&gt;=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow&gt;=0.15.0-&gt;isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.8.19.20240106)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n\n\nimport tensorflow as tf\nprint(tf.__version__)\n\n2.16.1\n\n\n\ntry:\n  import google.colab\n  IN_COLAB = True\nexcept:\n  IN_COLAB = False\n\nIN_COLAB\n\nFalse\n\n\n\n# sometime i run this notebook on my laptop for testing purpose\nlocal_dir = (\n    'datasets/images/' if IN_COLAB == False\n    else '/tmp/'\n)\n\ndataset_name = 'rockpaperscissors'\nlocal_data = local_dir + dataset_name\nlocal_zip = local_data + '.zip'\nprint(local_zip)\n\ndatasets/images/rockpaperscissors.zip\n\n\n\n!test -f $local_zip || wget --no-check-certificate \\\n https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \\\n -o $local_zip\n\n\nimport zipfile, os, shutil, splitfolders, re\n\n\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall(local_dir)\nzip_ref.close()\n\n\nshutil.rmtree(local_data + '/rps-cv-images')\nos.remove(local_data + '/README_rpc-cv-images.txt')\nprint(os.listdir(local_data))\n\nbase_dir = local_dir + '/rps'\nif os.path.exists(base_dir) == True:\n  shutil.rmtree(base_dir)\n\nsplitfolders.ratio(local_data, base_dir, ratio=(.6,.4))\ntrain_dir = os.path.join(base_dir, 'train')\nprint()\nprint('amount of training sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/train')))\nvalidation_dir = os.path.join(base_dir, 'val')\nprint('amount of validation sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/val')))\n\n['paper', 'rock', 'scissors']\n\namount of training sample :  1312\namount of validation sample :  876\n\n\nCopying files: 2188 files [00:00, 5444.71 files/s]\n\n\n\nos.listdir(train_dir)\n\n['paper', 'rock', 'scissors']\n\n\n\nos.listdir(validation_dir)\n\n['paper', 'rock', 'scissors']\n\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n    rotation_range = 20,\n    horizontal_flip = True,\n    shear_range = 0.2,\n    fill_mode = 'nearest',\n    preprocessing_function = preprocess_input,\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale = 1./255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (224,224),\n    batch_size = 4,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    batch_size = 4,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\nFound 1312 images belonging to 3 classes.\nFound 876 images belonging to 3 classes.\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom imutils import paths\nimport numpy as np\nimport pickle\nimport random\nimport os\nprint(\"[INFO] loading network...\")\nmodel = VGG16(weights=\"imagenet\", include_top=False)\ntrain_features = model.predict(train_generator)\ntrain_features = train_features.reshape((train_features.shape[0], 7 * 7 * 512))\nval_features = model.predict(validation_generator)\nval_features = val_features.reshape((val_features.shape[0], 7 * 7 * 512))\n\n[INFO] loading network...\n  2/328 ━━━━━━━━━━━━━━━━━━━━ 45s 141ms/step328/328 ━━━━━━━━━━━━━━━━━━━━ 54s 144ms/step\n219/219 ━━━━━━━━━━━━━━━━━━━━ 32s 146ms/step\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718172438.489068     309 service.cc:145] XLA service 0x7869d00032a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1718172438.489112     309 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\nI0000 00:00:1718172445.254437     309 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n\n\ntrain_features\n\narray([[0.15979376, 0.        , 0.        , ..., 0.        , 0.6380874 ,\n        0.        ],\n       [0.14911175, 0.        , 0.        , ..., 0.00925446, 0.84793645,\n        0.        ],\n       [0.1327081 , 0.        , 0.        , ..., 0.1794551 , 0.6249763 ,\n        0.        ],\n       ...,\n       [0.16579124, 0.        , 0.        , ..., 0.        , 0.5942712 ,\n        0.        ],\n       [0.09918394, 0.        , 0.        , ..., 0.        , 0.8006563 ,\n        0.        ],\n       [0.2933985 , 0.        , 0.        , ..., 0.        , 0.6294993 ,\n        0.        ]], dtype=float32)\n\n\n\ntrain_data = train_features.reshape(1312,7,14,256)\ntrain_data.shape\n\n(1312, 7, 14, 256)\n\n\n\n# import the necessary packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport pickle\nimport os\n\ntrainX, trainY = train_features, train_generator.classes\ntestX, testY = val_features, validation_generator.classes\n\n# train the model\nprint(\"[INFO] training model...\")\nmodelLogReg = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\",\n    max_iter=150)\nmodelLogReg.fit(trainX, trainY)\n# evaluate the model\nprint(\"[INFO] evaluating...\")\npreds = modelLogReg.predict(testX)\nprint(classification_report(testY, preds, target_names=train_generator.class_indices.keys()))\n\n[INFO] training model...\n[INFO] evaluating...\n              precision    recall  f1-score   support\n\n       paper       0.98      0.99      0.99       285\n        rock       0.98      1.00      0.99       291\n    scissors       1.00      0.98      0.99       300\n\n    accuracy                           0.99       876\n   macro avg       0.99      0.99      0.99       876\nweighted avg       0.99      0.99      0.99       876\n\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\nimport os\nimport numpy as np\n#from google.colab import files\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\ntestdir = 'datasets/images/rps/test/'\nuploaded = os.listdir(testdir)\nfig = plt.figure(figsize= (10, 10))\n#fig.tight_layout(pad=5.0)\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nfor i in range(len(uploaded)):\n    path = testdir + uploaded[i]\n    img = image.load_img(path, target_size = (224,224))\n\n    ax = fig.add_subplot(4, 4, i+1)\n    ax.imshow(img)\n    images = img_to_array(img)\n    images = np.expand_dims(images, axis=0)\n    images = preprocess_input(images)\n    #images = np.vstack([x])\n    features = model.predict(images, batch_size=4)\n    features = features.reshape((features.shape[0], 7 * 7 * 512))\n    pred = modelLogReg.predict(features)\n    ax.title.set_text(list(train_generator.class_indices.keys())[int(pred[0])])\n\nFileNotFoundError: [Errno 2] No such file or directory: 'datasets/images/rps/test/'\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "**Student Name** : Sulthan A. Karimov"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/ML - Unsupervised Learning.html",
    "href": "posts/Data Science/Machine Learning/ML - Unsupervised Learning.html",
    "title": "Sulthan A. Karimov",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv('datasets/mall_customers/Mall_Customers.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\n# change column name\ndf.columns = ['customer_id','gender', 'age', 'annual_income', 'spending_score']\n\ndf['gender'] = df['gender'].replace(['Female', 'Male'], [0,1])\n\n/tmp/ipykernel_2050756/2628508463.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df['gender'] = df['gender'].replace(['Female', 'Male'], [0,1])\n\n\n\ndf\n\n\n\n\n\n\n\n\ncustomer_id\ngender\nage\nannual_income\nspending_score\n\n\n\n\n0\n1\n1\n19\n15\n39\n\n\n1\n2\n1\n21\n15\n81\n\n\n2\n3\n0\n20\n16\n6\n\n\n3\n4\n0\n23\n16\n77\n\n\n4\n5\n0\n31\n17\n40\n\n\n...\n...\n...\n...\n...\n...\n\n\n195\n196\n0\n35\n120\n79\n\n\n196\n197\n0\n45\n126\n28\n\n\n197\n198\n1\n32\n126\n74\n\n\n198\n199\n1\n32\n137\n18\n\n\n199\n200\n1\n30\n137\n83\n\n\n\n\n200 rows × 5 columns\n\n\n\n\nfrom sklearn.cluster import KMeans\n\nX = df.drop(['customer_id', 'gender'], axis = 1)\n\nclusters = []\nfor i in range(1,11):\n    km = KMeans(n_clusters = i).fit(X)\n    clusters.append(km.inertia_)\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize = (8,4))\nsns.lineplot(x=list(range(1,11)), y=clusters, ax=ax)\nax.set_title('find elbow')\nax.set_xlabel('clusters')\nax.set_ylabel('inertia')\n\nText(0, 0.5, 'inertia')\n\n\n\n\n\n\n\n\n\n\nkm5 = KMeans(n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\n\nplt.figure(figsize=(8,4))\nsns.scatterplot(x=X['annual_income'], y=X['spending_score'], hue=X['Labels'],\n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans dengan 5 cluster')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "ML - Unsupervised Learning"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Submission.html",
    "href": "posts/Data Science/Machine Learning/Submission.html",
    "title": "Student Name : Sulthan A. Karimov",
    "section": "",
    "text": "Student Name : Sulthan A. Karimov\n\n\nUsername : sulthankarimov\n\n\nEmail : sulthankarimov@gmail.com\n\n!pip install split-folders[full] matplotlib jupyter_http_over_ws imutils\n\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.3)\nRequirement already satisfied: jupyter_http_over_ws in /usr/local/lib/python3.11/dist-packages (0.0.8)\nRequirement already satisfied: imutils in /usr/local/lib/python3.11/dist-packages (0.5.4)\nRequirement already satisfied: split-folders[full] in /usr/local/lib/python3.11/dist-packages (0.5.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from split-folders[full]) (4.66.4)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: notebook&gt;=5.0 in /usr/local/lib/python3.11/dist-packages (from jupyter_http_over_ws) (7.1.1)\nRequirement already satisfied: six&gt;=1.6.0 in /usr/lib/python3/dist-packages (from jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: tornado&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from jupyter_http_over_ws) (6.4)\nRequirement already satisfied: jupyter-server&lt;3,&gt;=2.4.0 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.13.0)\nRequirement already satisfied: jupyterlab-server&lt;3,&gt;=2.22.1 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.25.3)\nRequirement already satisfied: jupyterlab&lt;4.2,&gt;=4.1.1 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.1.4)\nRequirement already satisfied: notebook-shim&lt;0.3,&gt;=0.2 in /usr/local/lib/python3.11/dist-packages (from notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.4)\nRequirement already satisfied: anyio&gt;=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.3.0)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.1.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.1.3)\nRequirement already satisfied: jupyter-client&gt;=7.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (8.6.0)\nRequirement already satisfied: jupyter-core!=5.0.*,&gt;=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.7.1)\nRequirement already satisfied: jupyter-events&gt;=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.5.2)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (7.16.2)\nRequirement already satisfied: nbformat&gt;=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.9.2)\nRequirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (7.7.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.20.0)\nRequirement already satisfied: pyzmq&gt;=24 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (25.1.2)\nRequirement already satisfied: send2trash&gt;=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.2)\nRequirement already satisfied: terminado&gt;=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.0)\nRequirement already satisfied: traitlets&gt;=5.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.14.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.7.0)\nRequirement already satisfied: async-lru&gt;=1.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.4)\nRequirement already satisfied: httpx&gt;=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.27.0)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.29.3)\nRequirement already satisfied: jupyter-lsp&gt;=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.2.4)\nRequirement already satisfied: babel&gt;=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.14.0)\nRequirement already satisfied: json5&gt;=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.22)\nRequirement already satisfied: jsonschema&gt;=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.21.1)\nRequirement already satisfied: requests&gt;=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.31.0)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.6)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.0.4)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*-&gt;httpx&gt;=0.25.0-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.14.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.1.5)\nRequirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.33.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.18.0)\nRequirement already satisfied: platformdirs&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,&gt;=4.12-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.2.0)\nRequirement already satisfied: python-json-logger&gt;=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.7)\nRequirement already satisfied: pyyaml&gt;=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.0.1)\nRequirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.4)\nRequirement already satisfied: rfc3986-validator&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.12.3)\nRequirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (6.1.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.1)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.3.0)\nRequirement already satisfied: mistune&lt;4,&gt;=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.0.2)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.9.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.5.1)\nRequirement already satisfied: pygments&gt;=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.17.2)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.2.1)\nRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.11/dist-packages (from nbformat&gt;=5.3.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.19.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.31-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.31-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.2.1)\nRequirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado&gt;=0.8.3-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (21.2.0)\nRequirement already satisfied: comm&gt;=0.1.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.1)\nRequirement already satisfied: debugpy&gt;=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.8.1)\nRequirement already satisfied: ipython&gt;=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (8.22.2)\nRequirement already satisfied: matplotlib-inline&gt;=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.1.6)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.9.8)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.5.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.19.1)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (3.0.43)\nRequirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.6.3)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.11/dist-packages (from ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (4.9.0)\nRequirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.5.1)\nRequirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (20.11.0)\nRequirement already satisfied: jsonpointer&gt;1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.4)\nRequirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.0)\nRequirement already satisfied: webcolors&gt;=1.11 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.13)\nRequirement already satisfied: cffi&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.5)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.21)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi&gt;=0.16-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.8.3)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.13)\nRequirement already satisfied: arrow&gt;=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (1.3.0)\nRequirement already satisfied: executing&gt;=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.0.1)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.4.1)\nRequirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data-&gt;ipython&gt;=7.23.1-&gt;ipykernel-&gt;jupyterlab&lt;4.2,&gt;=4.1.1-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (0.2.2)\nRequirement already satisfied: types-python-dateutil&gt;=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow&gt;=0.15.0-&gt;isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=5.0-&gt;jupyter_http_over_ws) (2.8.19.20240106)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n# import all needed libraries\nimport zipfile, os, shutil, splitfolders, re, random\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom imutils import paths\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.backend import clear_session\nfrom sklearn.metrics import classification_report\n\n2024-06-13 06:38:22.014324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n# defining directories\nlocal_dir = 'datasets/images/'\ndataset_name = 'rockpaperscissors'\nlocal_data = local_dir + dataset_name\nlocal_zip = local_data + '.zip'\n\n\n# dowload dataset (pass if exist)\n!test -f $local_zip || wget --no-check-certificate \\\n https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \\\n -O $local_zip\n\n\n# extract dataset\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall(local_dir)\nzip_ref.close()\n\n\n# prepare train & validation split\nshutil.rmtree(local_data + '/rps-cv-images')\nos.remove(local_data + '/README_rpc-cv-images.txt')\nprint(os.listdir(local_data))\n\nbase_dir = local_dir + '/rps'\nif os.path.exists(base_dir) == True:\n  shutil.rmtree(base_dir)\n\nsplitfolders.ratio(local_data, base_dir, ratio=(.6,.4))\ntrain_dir = os.path.join(base_dir, 'train')\nprint('amount of training sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/train')))\nvalidation_dir = os.path.join(base_dir, 'val')\nprint('amount of validation sample : ', sum(len(files) for _, _, files in os.walk(re.escape(base_dir) + r'/val')))\n\n['paper', 'rock', 'scissors']\namount of training sample :  1312\namount of validation sample :  876\n\n\nCopying files: 2188 files [00:00, 5302.33 files/s]\n\n\n\n# prepare test folder for classification report\nfiles_list = []\n\nfor root, dirs, files in os.walk(local_data):\n  for dir in dirs:\n    results = os.walk(local_data + '/' + dir)\n    for result in results:\n      cat_list = []\n      for file in result[2]:\n        if file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"):\n          cat_list.append(os.path.join(root + '/' + dir, file))\n    files_list.append(cat_list)\n\nfor imgs in files_list:\n  category = re.findall('(?&lt;=rs\\/).*?(?=\\/)', imgs[0])[0]\n  filesToCopy = random.sample(imgs, 4)\n  destPath = base_dir + '/test/' + str(category)\n  if os.path.isdir(destPath) == False:\n    os.makedirs(destPath)\n  for file in filesToCopy:\n    shutil.copy(file, destPath)\n\ntest_dir = os.path.join(base_dir, 'test')\n\n\n# directory summary\nprint(os.listdir(train_dir))\nprint(os.listdir(validation_dir))\nprint(os.listdir(test_dir))\n\n['paper', 'rock', 'scissors']\n['paper', 'rock', 'scissors']\n['paper', 'rock', 'scissors']\n\n\n\n# preparing generator\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n    rotation_range = 20,\n    horizontal_flip = True,\n    shear_range = 0.2,\n    fill_mode = 'nearest',\n    preprocessing_function = preprocess_input,\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale = 1./255)\n\n\n# defining rgb mean for every generator\nmean = np.array([123.68, 116.779, 103.939], dtype = 'float32')\ntrain_datagen.mean = mean\ntest_datagen.mean = mean\n\n\n# flow data to generator\nBATCH_SIZE = 32\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    shuffle = False,\n)\n\ntotalTrain = len(list(paths.list_images(train_dir)))\ntotalVal = len(list(paths.list_images(validation_dir)))\ntotalTest = len(list(paths.list_images(test_dir)))\n\nFound 1312 images belonging to 3 classes.\nFound 876 images belonging to 3 classes.\nFound 12 images belonging to 3 classes.\n\n\n\n# preparing baseModel\nbaseModel = VGG16(weights=\"imagenet\", include_top=False,\n    input_tensor=Input(shape=(224, 224, 3)))\nheadModel = baseModel.output\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(512, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(3, activation=\"softmax\")(headModel)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n2024-06-13 06:38:33.026430: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.036228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.036873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.038422: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.039011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.039222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.100423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.100868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.101216: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-13 06:38:33.101481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3503 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n\n\n\n# freeze hidden layers to preserve model features\nfor layer in baseModel.layers:\n    layer.trainable = False\n\n\n# prepare callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nEarlyStop = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 4,\n    verbose = 1,\n    restore_best_weights = True,\n    min_delta = 0.1\n)\n\nModelCP = ReduceLROnPlateau(\n    monitor = 'val_loss',\n    factor = 0.5,\n    patience = 1,\n    verbose = 1\n)\n\ncallbacks = [EarlyStop, ModelCP]\n\n\n# compile the model with frozen layers\nprint(\"[INFO] compiling model...\")\nopt = SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\nprint(\"[INFO] training head...\")\nH = model.fit(\n    x=train_generator,\n    steps_per_epoch=totalTrain // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=totalVal // BATCH_SIZE,\n    epochs=25,\n    callbacks=callbacks,\n)\n\n[INFO] compiling model...\n[INFO] training head...\nEpoch 1/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 65s 764ms/step - accuracy: 0.3194 - loss: 1.6568 - val_accuracy: 0.3299 - val_loss: 1.1905 - learning_rate: 1.0000e-04\nEpoch 2/25\n\nEpoch 2: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 13s 324ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 2.0495 - learning_rate: 1.0000e-04\nEpoch 3/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 34s 762ms/step - accuracy: 0.4824 - loss: 1.1331 - val_accuracy: 0.6551 - val_loss: 0.8608 - learning_rate: 5.0000e-05\nEpoch 4/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8333 - val_loss: 0.7623 - learning_rate: 5.0000e-05\nEpoch 5/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 471ms/step - accuracy: 0.5609 - loss: 0.9040\nEpoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 35s 789ms/step - accuracy: 0.5616 - loss: 0.9034 - val_accuracy: 0.3414 - val_loss: 0.9531 - learning_rate: 5.0000e-05\nEpoch 6/25\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 1.4488 - learning_rate: 2.5000e-05\nEpoch 7/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 37s 851ms/step - accuracy: 0.4162 - loss: 1.0887 - val_accuracy: 0.9155 - val_loss: 0.7140 - learning_rate: 1.2500e-05\nEpoch 8/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.6769 - learning_rate: 1.2500e-05\nEpoch 9/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 553ms/step - accuracy: 0.8038 - loss: 0.7564\nEpoch 9: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 39s 879ms/step - accuracy: 0.8039 - loss: 0.7561 - val_accuracy: 0.8727 - val_loss: 0.6917 - learning_rate: 1.2500e-05\nEpoch 10/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.6729 - learning_rate: 6.2500e-06\nEpoch 11/25\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 539ms/step - accuracy: 0.7804 - loss: 0.7520\nEpoch 11: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 38s 869ms/step - accuracy: 0.7808 - loss: 0.7515 - val_accuracy: 0.8692 - val_loss: 0.6797 - learning_rate: 6.2500e-06\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 7.\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718260718.096354    2416 service.cc:145] XLA service 0x7e2f6c00bac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1718260718.096393    2416 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\n2024-06-13 06:38:38.140178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-06-13 06:38:38.387163: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n2024-06-13 06:38:42.864229: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2024-06-13 06:38:42.864269: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2024-06-13 06:38:43.727778: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2024-06-13 06:38:44.727981: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng11{k2=1,k3=0} for conv (f32[32,64,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,224,224]{3,2,1,0}, f32[64,64,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:45.144180: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.416328597s\nTrying algorithm eng11{k2=1,k3=0} for conv (f32[32,64,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,224,224]{3,2,1,0}, f32[64,64,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:49.990206: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng36{k2=4,k3=0} for conv (f32[32,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:51.947501: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.957386015s\nTrying algorithm eng36{k2=4,k3=0} for conv (f32[32,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:52.947650: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng36{k2=3,k3=0} for conv (f32[32,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:53.103744: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.156164154s\nTrying algorithm eng36{k2=3,k3=0} for conv (f32[32,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-06-13 06:38:54.454487: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\nI0000 00:00:1718260747.621493    2416 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n2024-06-13 06:39:38.617402: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:39:38.617449: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:39:38.617466: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:39:38.617484: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n2024-06-13 06:39:51.896922: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:39:51.896973: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:39:51.896987: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:39:51.897001: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:40:25.878261: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:40:25.878307: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:40:25.878325: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:40:25.878349: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:40:26.110588: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:40:26.110637: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:40:26.110670: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:40:26.110696: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:41:01.193988: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:41:01.194027: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:41:01.433249: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:41:01.433288: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:41:01.433300: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:41:01.433317: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:41:38.865435: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:41:38.865565: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:41:39.121906: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:41:39.121993: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:42:17.951701: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:42:17.951753: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:42:17.951777: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:42:17.951811: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:42:18.198832: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:42:18.198885: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:42:18.198902: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:42:18.198925: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n\n\n\n# create classification_report for trained model\nprint(\"[INFO] evaluating after fine-tuning network head...\")\ntest_generator.reset()\npredIdxs = model.predict(x=test_generator,\n    steps=(totalTest // BATCH_SIZE) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(test_generator.classes, predIdxs,\n    target_names=test_generator.class_indices.keys()))\n\n[INFO] evaluating after fine-tuning network head...\n1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 523ms/step\n              precision    recall  f1-score   support\n\n       paper       1.00      1.00      1.00         4\n        rock       1.00      0.75      0.86         4\n    scissors       0.80      1.00      0.89         4\n\n    accuracy                           0.92        12\n   macro avg       0.93      0.92      0.92        12\nweighted avg       0.93      0.92      0.92        12\n\n\n\n\n# recreate model with unfrozen layers\nclear_session()\ntrain_generator.reset()\nvalidation_generator.reset()\nfor layer in baseModel.layers[15:]:\n    layer.trainable = True\n\n\n# recompile and retrain the model after unfreezing layers\nprint(\"[INFO] re-compiling model...\")\nopt = SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\nH = model.fit(\n    x=train_generator,\n    steps_per_epoch=totalTrain // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=totalVal // BATCH_SIZE,\n    epochs=20,\n    callbacks=callbacks,\n)\n\n[INFO] re-compiling model...\nEpoch 1/20\n41/41 ━━━━━━━━━━━━━━━━━━━━ 41s 841ms/step - accuracy: 0.8991 - loss: 0.3942 - val_accuracy: 0.8449 - val_loss: 0.3809 - learning_rate: 1.0000e-04\nEpoch 2/20\n41/41 ━━━━━━━━━━━━━━━━━━━━ 1s 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1072 - learning_rate: 1.0000e-04\nEpoch 3/20\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 540ms/step - accuracy: 0.8902 - loss: 0.3041\nEpoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 39s 861ms/step - accuracy: 0.8907 - loss: 0.3038 - val_accuracy: 0.9537 - val_loss: 0.1826 - learning_rate: 1.0000e-04\nEpoch 4/20\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1245 - learning_rate: 5.0000e-05\nEpoch 5/20\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 638ms/step - accuracy: 0.9678 - loss: 0.1691\nEpoch 5: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 43s 972ms/step - accuracy: 0.9676 - loss: 0.1694 - val_accuracy: 0.9595 - val_loss: 0.1536 - learning_rate: 2.5000e-05\nEpoch 6/20\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n41/41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1329 - learning_rate: 1.2500e-05\nEpoch 6: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n\n2024-06-13 06:53:29.167122: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n2024-06-13 06:53:29.167245: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:53:29.745362: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:53:29.745418: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:53:29.745443: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:54:08.699110: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:54:08.699156: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:54:08.943815: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:54:08.943913: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_4]]\n2024-06-13 06:54:52.113182: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:54:52.113229: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_4]]\n2024-06-13 06:54:52.113261: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n2024-06-13 06:54:52.364841: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n2024-06-13 06:54:52.364891: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n     [[{{node IteratorGetNext}}]]\n     [[IteratorGetNext/_2]]\n2024-06-13 06:54:52.364915: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 1113105719973859051\n2024-06-13 06:54:52.364932: I tensorflow/core/framework/local_rendezvous.cc:422] Local rendezvous recv item cancelled. Key hash: 17891322345123413466\n\n\n\n# create classification_report for full model\nprint(\"[INFO] evaluating after fine-tuning network...\")\ntest_generator.reset()\npredIdxs = model.predict(x=test_generator,\n    steps=(totalTest // BATCH_SIZE) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(test_generator.classes, predIdxs,\n    target_names=test_generator.class_indices.keys()))\n\n[INFO] evaluating after fine-tuning network...\n1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 512ms/step\n              precision    recall  f1-score   support\n\n       paper       1.00      1.00      1.00         4\n        rock       1.00      1.00      1.00         4\n    scissors       1.00      1.00      1.00         4\n\n    accuracy                           1.00        12\n   macro avg       1.00      1.00      1.00        12\nweighted avg       1.00      1.00      1.00        12\n\n\n\n\nimport os\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\ntestdir = test_dir\nuploaded = list(paths.list_images(testdir))\nfig = plt.figure(figsize= (10, 10))\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1,\n                    right=0.9,\n                    top=0.9,\n                    wspace=0.4,\n                    hspace=0.4)\nfor i in range(len(uploaded)):\n    path = uploaded[i]\n    img = image.load_img(path, target_size = (224,224))\n\n    ax = fig.add_subplot(4, 4, i+1)\n    ax.imshow(img)\n    images = image.img_to_array(img)\n    images = np.expand_dims(images, axis=0)\n    images = preprocess_input(images)\n    pred = model.predict(images)\n    ax.title.set_text(list(train_generator.class_indices.keys())[np.argmax(pred, axis = 1)[0]])\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "**Student Name** : Sulthan A. Karimov"
    ]
  },
  {
    "objectID": "posts/Data Science/Machine Learning/Transfer Learning with CNN.html",
    "href": "posts/Data Science/Machine Learning/Transfer Learning with CNN.html",
    "title": "Sulthan A. Karimov",
    "section": "",
    "text": "# import the necessary packages\nimport os\n# initialize the path to the *original* input directory of images\nORIG_INPUT_DATASET = \"Food-5K\"\n# initialize the base path to the *new* directory that will contain\n# our images after computing the training and testing split\nBASE_PATH = \"datasets/images/rps\"\n# define the names of the training, testing, and validation\n# directories\nTRAIN = \"train\"\nTEST = \"test\"\nVAL = \"val\"\n# initialize the list of class label names\nCLASSES = [\"rock\", \"paper\", \"scissors\"]\n# set the batch size\nBATCH_SIZE = 32\n# initialize the label encoder file path and the output directory to\n# where the extracted features (in CSV file format) will be stored\nLE_PATH = os.path.sep.join([\"rpsoutput2\", \"le.cpickle\"])\nBASE_CSV_PATH = \"rpsoutput2\"\n\n\n# import the necessary packages\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom imutils import paths\nimport numpy as np\nimport pickle\nimport random\nimport os\n# load the ResNet50 network and initialize the label encoder\nprint(\"[INFO] loading network...\")\nmodel = VGG16(weights=\"imagenet\", include_top=False)\nle = None\n# loop over the data splits\nfor split in (TRAIN, TEST, VAL):\n    # grab all image paths in the current split\n    print(\"[INFO] processing '{} split'...\".format(split))\n    p = os.path.sep.join([BASE_PATH, split])\n    imagePaths = list(paths.list_images(p))\n    # randomly shuffle the image paths and then extract the class\n    # labels from the file paths\n    random.shuffle(imagePaths)\n    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n    # if the label encoder is None, create it\n    if le is None:\n        le = LabelEncoder()\n        le.fit(labels)\n    # open the output CSV file for writing\n    csvPath = os.path.sep.join([BASE_CSV_PATH,\n        \"{}.csv\".format(split)])\n    csv = open(csvPath, \"w\")\n    # loop over the images in batches\n    for (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n        # extract the batch of images and labels, then initialize the\n        # list of actual images that will be passed through the network\n        # for feature extraction\n        print(\"[INFO] processing batch {}/{}\".format(b + 1,\n            int(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n        batchPaths = imagePaths[i:i + BATCH_SIZE]\n        batchLabels = le.transform(labels[i:i + BATCH_SIZE])\n        batchImages = []\n        # loop over the images and labels in the current batch\n        for imagePath in batchPaths:\n            # load the input image using the Keras helper utility\n            # while ensuring the image is resized to 224x224 pixels\n            image = load_img(imagePath, target_size=(224, 224))\n            image = img_to_array(image)\n            # preprocess the image by (1) expanding the dimensions and\n            # (2) subtracting the mean RGB pixel intensity from the\n            # ImageNet dataset\n            image = np.expand_dims(image, axis=0)\n            \n            image = preprocess_input(image)\n            # add the image to the batch\n            batchImages.append(image)\n        # pass the images through the network and use the outputs as\n        # our actual features, then reshape the features into a\n        # flattened volume\n        batchImages = np.vstack(batchImages)\n        features = model.predict(batchImages, batch_size=BATCH_SIZE)\n        features = features.reshape((features.shape[0], 7 * 7 * 512))\n        # loop over the class labels and extracted features\n        for (label, vec) in zip(batchLabels, features):\n            # construct a row that exists of the class label and\n            # extracted features\n            vec = \",\".join([str(v) for v in vec])\n            csv.write(\"{},{}\\n\".format(label, vec))\n    # close the CSV file\n    csv.close()\n# serialize the label encoder to disk\nf = open(LE_PATH, \"wb\")\nf.write(pickle.dumps(le))\nf.close()\n\n[INFO] loading network...\n[INFO] processing 'train split'...\n[INFO] processing batch 1/41\n\n\n2024-06-12 15:58:34.912300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:34.924843: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:34.925545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:34.927471: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:34.928240: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:34.928610: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:35.002875: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:35.003115: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:35.003314: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2024-06-12 15:58:35.003476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 226 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718207915.851926     803 service.cc:145] XLA service 0x77f444104fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1718207915.851960     803 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\n2024-06-12 15:58:35.863848: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-06-12 15:58:35.970248: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n2024-06-12 15:58:35.991171: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 408.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2024-06-12 15:58:35.993817: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:580 : UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.39 = (f32[32,64,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,3,224,224]{3,2,1,0} %transpose.30, f32[64,3,3,3]{3,2,1,0} %transpose.31, f32[64]{0} %arg2.3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"vgg16_1/block1_conv1_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1177}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 427819008 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n2024-06-12 15:58:35.993865: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.39 = (f32[32,64,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,3,224,224]{3,2,1,0} %transpose.30, f32[64,3,3,3]{3,2,1,0} %transpose.31, f32[64]{0} %arg2.3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"vgg16_1/block1_conv1_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1177}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 427819008 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n     [[{{node StatefulPartitionedCall}}]]\n\n\nUnknownError: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in &lt;module&gt;\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_689/3128301997.py\", line 63, in &lt;module&gt;\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 118, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 513, in predict\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 212, in one_step_on_data_distributed\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.39 = (f32[32,64,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,3,224,224]{3,2,1,0} %transpose.30, f32[64,3,3,3]{3,2,1,0} %transpose.31, f32[64]{0} %arg2.3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"vgg16_1/block1_conv1_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1177}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 427819008 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n     [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_data_distributed_502]\n\n\n\n# import the necessary packages\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport pickle\nimport os\n\ndef csv_feature_generator(inputPath, bs, numClasses, mode=\"train\"):\n    # open the input file for reading\n    f = open(inputPath, \"r\")\n    # loop indefinitely\n    while True:\n        # initialize our batch of data and labels\n        data = []\n        labels = []\n        # keep looping until we reach our batch size\n        while len(data) &lt; bs:\n            # attempt to read the next row of the CSV file\n            row = f.readline()\n            # check to see if the row is empty, indicating we have\n            # reached the end of the file\n            if row == \"\":\n                # reset the file pointer to the beginning of the file\n                # and re-read the row\n                f.seek(0)\n                row = f.readline()\n                # if we are evaluating we should now break from our\n                # loop to ensure we don't continue to fill up the\n                # batch from samples at the beginning of the file\n                if mode == \"eval\":\n                    break\n            # extract the class label and features from the row\n            row = row.strip().split(\",\")\n            label = row[0]\n            label = to_categorical(label, num_classes=numClasses)\n            features = np.array(row[1:], dtype=\"float\")\n            # update the data and label lists\n            data.append(features)\n            labels.append(label)\n        # yield the batch to the calling function\n        print(np.array(data).shape)\n        yield (np.array(data), np.array(labels))\n# load the label encoder from disk\nle = pickle.loads(open(LE_PATH, \"rb\").read())\n# derive the paths to the training, validation, and testing CSV files\ntrainPath = os.path.sep.join([BASE_CSV_PATH,\n                              \"{}.csv\".format(TRAIN)])\nvalPath = os.path.sep.join([BASE_CSV_PATH,\n                            \"{}.csv\".format(VAL)])\ntestPath = os.path.sep.join([BASE_CSV_PATH,\n                             \"{}.csv\".format(TEST)])\n# determine the total number of images in the training and validation\n# sets\ntotalTrain = sum([1 for l in open(trainPath)])\ntotalVal = sum([1 for l in open(valPath)])\n# extract the testing labels from the CSV file and then determine the\n# number of testing images\ntestLabels = [int(row.split(\",\")[0]) for row in open(testPath)]\ntotalTest = len(testLabels)\n# construct the training, validation, and testing generators\ntrainGen = csv_feature_generator(trainPath, BATCH_SIZE,\n                                 len(CLASSES), mode=\"train\")\nvalGen = csv_feature_generator(valPath, BATCH_SIZE,\n                               len(CLASSES), mode=\"eval\")\ntestGen = csv_feature_generator(testPath, BATCH_SIZE,\n                                len(CLASSES), mode=\"eval\")\n\n\ndir(trainGen)\n\n['__class__',\n '__del__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__lt__',\n '__name__',\n '__ne__',\n '__new__',\n '__next__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'close',\n 'gi_code',\n 'gi_frame',\n 'gi_running',\n 'gi_suspended',\n 'gi_yieldfrom',\n 'send',\n 'throw']\n\n\n\n# define our simple neural network\nmodel = Sequential()\nmodel.add(Input(shape = (7*7*512,)))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dense(len(CLASSES), activation=\"softmax\"))\n# compile the model\nopt = SGD(learning_rate=1e-3, momentum=0.9, weight_decay=1e-3 / 25)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n    metrics=[\"accuracy\"])\n# train the network\nprint(\"[INFO] training simple network...\")\nH = model.fit(\n    x=trainGen,\n    steps_per_epoch=totalTrain // BATCH_SIZE,\n    validation_data=valGen,\n    validation_steps=totalVal // BATCH_SIZE,\n    epochs=25)\n\n[INFO] training simple network...\n\n\nValueError: invalid literal for int() with base 10: ''\n\n\n\n# make predictions on the testing images, finding the index of the\n# label with the corresponding largest predicted probability, then\n# show a nicely formatted classification report\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(x=testGen,\n    steps=(totalTest //BATCH_SIZE) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(testLabels, predIdxs,\n    target_names=le.classes_))\n\n[INFO] evaluating network...\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 219ms/step\n              precision    recall  f1-score   support\n\n       paper       1.00      1.00      1.00         4\n        rock       1.00      1.00      1.00         4\n    scissors       1.00      1.00      1.00         4\n\n    accuracy                           1.00        12\n   macro avg       1.00      1.00      1.00        12\nweighted avg       1.00      1.00      1.00        12\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Posts",
      "Data Science",
      "Machine Learning",
      "Transfer Learning with CNN"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Just Another Attempt to Share\nIlmu memang memberi kita pengetahuan, tetapi hanya filsafat yang bisa memberi kita kebijaksanaan.\nWill Durant\n\n\n\n Back to top",
    "crumbs": [
      "About"
    ]
  }
]